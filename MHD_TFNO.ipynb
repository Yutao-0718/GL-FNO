{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de3253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from neuralop.models.fno import TFNO\n",
    "from neuralop.training.trainer import Trainer\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop.losses.data_losses import LpLoss, H1Loss\n",
    "import pdb\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from astropy.io import fits\n",
    "from torch.utils.data.dataset import Subset\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.transforms.functional import normalize\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f05eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, hdf5_file_path):\n",
    "        self.hdf5_file_path = hdf5_file_path\n",
    "        self.hdf5_file = h5py.File(hdf5_file_path, 'r')\n",
    "        self.dataset_length = len(self.hdf5_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'x': torch.from_numpy(self.hdf5_file[f'sample_{idx}/x'][:]),\n",
    "            'y': torch.from_numpy(self.hdf5_file[f'sample_{idx}/y'][:])\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "processed_data_path = \"/data.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dee22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed_value = 1\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Create the custom dataset\n",
    "custom_dataset = CustomDataset(hdf5_file_path=processed_data_path)\n",
    "\n",
    "# Define the size for the test set\n",
    "test_set_size = 20\n",
    "\n",
    "# Generate random indices for the test set and corresponding training set\n",
    "all_indices = list(range(len(custom_dataset)))\n",
    "random.shuffle(all_indices)\n",
    "\n",
    "test_indices = all_indices[:test_set_size]\n",
    "train_indices = all_indices[test_set_size:]\n",
    "\n",
    "# Create training dataset using Subset\n",
    "train_dataset = Subset(custom_dataset, train_indices)\n",
    "\n",
    "# Create testing dataset using Subset\n",
    "test_dataset = Subset(custom_dataset, test_indices)\n",
    "\n",
    "# Example usage in a DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0, persistent_workers=False)\n",
    "test_loaders = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=0, persistent_workers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a2b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loaders:\n",
    "    x_shape = batch['x'].shape\n",
    "    y_shape = batch['y'].shape\n",
    "    print(f'Batch X shape: {x_shape}')\n",
    "    print(f'Batch Y shape: {y_shape}')\n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b199565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and initialize TFNO model\n",
    "model = TFNO(n_modes=(64,64), in_channels=1, out_channels=99, hidden_channels=128, projection_channels=256, use_mlp=True,\n",
    "                              factorization='tucker', rank=0.5)\n",
    "model = model.to(device)\n",
    "\n",
    "n_params = count_model_params(model)\n",
    "print(f'\\nYour model has {n_params} parameters.')\n",
    "sys.stdout.flush()  # flush the stdout buffer\n",
    "h1loss = H1Loss(d=2)\n",
    "test_loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-4) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6541c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "epoch_durations = []\n",
    "\n",
    "num_epochs = 300\n",
    "print_frequency = 40\n",
    "save_checkpoint_interval = 300\n",
    "checkpoint_dir = \"/result/\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # training process\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data, target = sample['x'].to(device), sample['y'].to(device)\n",
    "        output = model(data)\n",
    "        loss = h1loss(output, target)\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "        if batch_idx % print_frequency == 0:\n",
    "            print(f'Train Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(train_loader)}, Loss: {loss.item()}')\n",
    "    \n",
    "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # testing process\n",
    "    model.eval()\n",
    "    epoch_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for sample in test_loaders:\n",
    "            data, target = sample['x'].to(device), sample['y'].to(device)\n",
    "            output = model(data)\n",
    "            loss = test_loss(output, target)\n",
    "            loss = loss.mean()\n",
    "            epoch_test_loss += loss.item()\n",
    "    \n",
    "    avg_test_loss = epoch_test_loss / len(test_loaders)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    epoch_durations.append(epoch_duration)\n",
    "\n",
    "    # Updated learning rate\n",
    "    scheduler.step(avg_train_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Duration: {epoch_duration:.2f}s, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "    # save checkpoint\n",
    "    if (epoch + 1) % save_checkpoint_interval == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved at epoch {epoch+1}: {checkpoint_path}\")\n",
    "\n",
    "# average time of epoch duration\n",
    "avg_epoch_duration = np.mean(epoch_durations)\n",
    "print(f'Average Epoch Duration: {avg_epoch_duration:.2f}s')\n",
    "\n",
    "# save loss\n",
    "np.save(os.path.join(checkpoint_dir, 'train_losses.npy'), np.array(train_losses))\n",
    "np.save(os.path.join(checkpoint_dir, 'test_losses.npy'), np.array(test_losses))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(test_losses, label='Testing Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Testing Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0981cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "sample_r2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample in test_loaders:\n",
    "        x = sample['x'].to(device)\n",
    "        y_true_batch = sample['y'].numpy()\n",
    "        y_pred_batch = model(x).cpu().numpy()\n",
    "\n",
    "        # Calculate R-squared for each sample in the batch\n",
    "        for true_sample, pred_sample in zip(y_true_batch, y_pred_batch):\n",
    "            sample_r2_scores = r2_score(true_sample.flatten(), pred_sample.flatten())\n",
    "            sample_r2.append(sample_r2_scores)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize an empty list to store all sample relative errors\n",
    "sample_relative_errors = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample in test_loaders:\n",
    "        x = sample['x'].to(device)\n",
    "        y_true_batch = sample['y'].numpy()\n",
    "        y_pred_batch = model(x).cpu().numpy()\n",
    "\n",
    "        # Calculate Relative Error for each sample in the batch\n",
    "        for true_sample, pred_sample in zip(y_true_batch, y_pred_batch):\n",
    "            absolute_error = np.linalg.norm(true_sample.flatten() - pred_sample.flatten(), 2)\n",
    "            relative_error = absolute_error / (np.linalg.norm(true_sample.flatten(), 2)) \n",
    "            #print(relative_error)\n",
    "            sample_relative_errors.append(relative_error)\n",
    "            \n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "min_mse_sample = None\n",
    "sample_mses = []  # Initialize outside the loop\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample_idx, sample in enumerate(test_loaders):\n",
    "        x = sample['x'].to(device)\n",
    "        y_true_batch = sample['y'].numpy()  \n",
    "        y_pred_batch = model(x).cpu().numpy()\n",
    "\n",
    "        # Iterate through each ground truth-prediction pair in the sample\n",
    "        for true_sample, pred_sample in zip(y_true_batch, y_pred_batch):\n",
    "            mse_sample = mean_squared_error(true_sample.flatten(), pred_sample.flatten())\n",
    "            sample_mses.append(mse_sample)\n",
    "\n",
    "# Calculate overall Mean Squared Error\n",
    "overall_mse = np.mean(sample_mses)\n",
    "print(f'Overall Mean Squared Error: {overall_mse}')\n",
    "\n",
    "# Find the minimum MSE value and its index\n",
    "min_sample_mse = min(sample_mses)\n",
    "min_index = np.argmin(sample_mses)\n",
    "\n",
    "#print(f'Index of Minimum MSE: {min_index}')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize an empty list to store all sample absolute errors\n",
    "sample_absolute_errors = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample in test_loaders:\n",
    "        x = sample['x'].to(device)\n",
    "        y_true_batch = sample['y'].numpy()\n",
    "        y_pred_batch = model(x).cpu().numpy()\n",
    "\n",
    "        # Calculate Absolute Error for each sample in the batch\n",
    "        for true_sample, pred_sample in zip(y_true_batch, y_pred_batch):\n",
    "            absolute_error = np.mean(np.abs(true_sample.flatten() - pred_sample.flatten()))\n",
    "            sample_absolute_errors.append(absolute_error)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = np.mean(sample_absolute_errors)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "\n",
    "# Calculate overall Relative Error\n",
    "overall_relative_error = np.mean(sample_relative_errors)\n",
    "print(f'Overall Relative Error: {overall_relative_error}')\n",
    "\n",
    "# Calculate overall R-squared\n",
    "overall_r2 = np.mean(sample_r2)\n",
    "print(f'Overall R-squared: {overall_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2b517c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from neuralop.models import TFNO, UNO\n",
    "from neuralop.training.trainer import Trainer\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop.losses.data_losses import LpLoss, H1Loss\n",
    "import pdb\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from astropy.io import fits\n",
    "from torch.utils.data.dataset import Subset\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.transforms.functional import normalize\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "import h5py\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9df527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from neuralop.layers.mlp import MLP\n",
    "from neuralop.layers.normalization_layers import AdaIN\n",
    "from neuralop.layers.skip_connections import skip_connection\n",
    "from neuralop.layers.spectral_convolution import SpectralConv\n",
    "from neuralop.utils import validate_scaling_factor\n",
    "\n",
    "class U_net(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, dropout_rate):\n",
    "        super(U_net, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.conv1 = self.conv(input_channels, output_channels, kernel_size=kernel_size, stride=2, dropout_rate=dropout_rate)\n",
    "        self.conv2 = self.conv(output_channels, output_channels, kernel_size=kernel_size, stride=2, dropout_rate=dropout_rate)\n",
    "        self.conv2_1 = self.conv(output_channels, output_channels, kernel_size=kernel_size, stride=1, dropout_rate=dropout_rate)\n",
    "        self.conv3 = self.conv(output_channels, output_channels, kernel_size=kernel_size, stride=2, dropout_rate=dropout_rate)\n",
    "        self.conv3_1 = self.conv(output_channels, output_channels, kernel_size=kernel_size, stride=1, dropout_rate=dropout_rate)\n",
    "        \n",
    "        self.deconv2 = self.deconv(output_channels, output_channels)\n",
    "        self.deconv1 = self.deconv(output_channels*2, output_channels)\n",
    "        self.deconv0 = self.deconv(output_channels*2, output_channels)\n",
    "    \n",
    "        self.output_layer = self.output(output_channels*2, output_channels, kernel_size=kernel_size, stride=1, dropout_rate=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_conv1 = self.conv1(x)\n",
    "        out_conv2 = self.conv2_1(self.conv2(out_conv1))\n",
    "        out_conv3 = self.conv3_1(self.conv3(out_conv2))\n",
    "        out_deconv2 = self.deconv2(out_conv3)\n",
    "        concat2 = torch.cat((out_conv2, out_deconv2), 1)\n",
    "        out_deconv1 = self.deconv1(concat2)\n",
    "        concat1 = torch.cat((out_conv1, out_deconv1), 1)\n",
    "        out_deconv0 = self.deconv0(concat1)\n",
    "        concat0 = torch.cat((x, out_deconv0), 1)\n",
    "        out = self.output_layer(concat0)\n",
    "        return out\n",
    "\n",
    "    def conv(self, in_planes, output_channels, kernel_size, stride, dropout_rate):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, output_channels, kernel_size=kernel_size,\n",
    "                      stride=stride, padding=(kernel_size - 1) // 2, bias=False),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def deconv(self, input_channels, output_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(input_channels, output_channels, kernel_size=4,\n",
    "                               stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.1, inplace=True)\n",
    "        )\n",
    "\n",
    "    def output(self, input_channels, output_channels, kernel_size, stride, dropout_rate):\n",
    "        return nn.Conv2d(input_channels, output_channels, kernel_size=kernel_size,\n",
    "                         stride=stride, padding=(kernel_size - 1) // 2)\n",
    "\n",
    "class FNOBlockswithUnet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        n_modes,\n",
    "        output_scaling_factor=None,\n",
    "        n_layers=1,\n",
    "        max_n_modes=None,\n",
    "        fno_block_precision=\"full\",\n",
    "        use_mlp=False,\n",
    "        mlp_dropout=0,\n",
    "        mlp_expansion=0.5,\n",
    "        non_linearity=F.gelu,\n",
    "        stabilizer=None,\n",
    "        norm=None,\n",
    "        ada_in_features=None,\n",
    "        preactivation=False,\n",
    "        fno_skip=\"linear\",\n",
    "        mlp_skip=\"soft-gating\",\n",
    "        separable=False,\n",
    "        factorization=None,\n",
    "        rank=1.0,\n",
    "        SpectralConv=SpectralConv,\n",
    "        joint_factorization=False,\n",
    "        fixed_rank_modes=False,\n",
    "        implementation=\"factorized\",\n",
    "        decomposition_kwargs=dict(),\n",
    "        fft_norm=\"forward\",\n",
    "        unet_kernel_size=3,\n",
    "        unet_dropout_rate=0.1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if isinstance(n_modes, int):\n",
    "            n_modes = [n_modes]\n",
    "        self._n_modes = n_modes\n",
    "        self.n_dim = len(n_modes)\n",
    "\n",
    "        self.output_scaling_factor = validate_scaling_factor(output_scaling_factor, self.n_dim, n_layers)\n",
    "\n",
    "        self.max_n_modes = max_n_modes\n",
    "        self.fno_block_precision = fno_block_precision\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.n_layers = n_layers\n",
    "        self.joint_factorization = joint_factorization\n",
    "        self.non_linearity = non_linearity\n",
    "        self.stabilizer = stabilizer\n",
    "        self.rank = rank\n",
    "        self.factorization = factorization\n",
    "        self.fixed_rank_modes = fixed_rank_modes\n",
    "        self.decomposition_kwargs = decomposition_kwargs\n",
    "        self.fno_skip = fno_skip\n",
    "        self.mlp_skip = mlp_skip\n",
    "        self.use_mlp = use_mlp\n",
    "        self.mlp_expansion = mlp_expansion\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        self.fft_norm = fft_norm\n",
    "        self.implementation = implementation\n",
    "        self.separable = separable\n",
    "        self.preactivation = preactivation\n",
    "        self.ada_in_features = ada_in_features\n",
    "\n",
    "        self.convs = SpectralConv(\n",
    "            self.in_channels,\n",
    "            self.out_channels,\n",
    "            self.n_modes,\n",
    "            output_scaling_factor=output_scaling_factor,\n",
    "            max_n_modes=max_n_modes,\n",
    "            rank=rank,\n",
    "            fixed_rank_modes=fixed_rank_modes,\n",
    "            implementation=implementation,\n",
    "            separable=separable,\n",
    "            factorization=factorization,\n",
    "            decomposition_kwargs=decomposition_kwargs,\n",
    "            joint_factorization=joint_factorization,\n",
    "            n_layers=n_layers,\n",
    "        )\n",
    "\n",
    "        self.fno_skips = nn.ModuleList(\n",
    "            [\n",
    "                skip_connection(\n",
    "                    self.in_channels,\n",
    "                    self.out_channels,\n",
    "                    skip_type=fno_skip,\n",
    "                    n_dim=self.n_dim,\n",
    "                )\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if use_mlp:\n",
    "            self.mlp = nn.ModuleList(\n",
    "                [\n",
    "                    MLP(\n",
    "                        in_channels=self.out_channels,\n",
    "                        hidden_channels=round(self.out_channels * mlp_expansion),\n",
    "                        dropout=mlp_dropout,\n",
    "                        n_dim=self.n_dim,\n",
    "                    )\n",
    "                    for _ in range(n_layers)\n",
    "                ]\n",
    "            )\n",
    "            self.mlp_skips = nn.ModuleList(\n",
    "                [\n",
    "                    skip_connection(\n",
    "                        self.in_channels,\n",
    "                        self.out_channels,\n",
    "                        skip_type=mlp_skip,\n",
    "                        n_dim=self.n_dim,\n",
    "                    )\n",
    "                    for _ in range(n_layers)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.mlp = None\n",
    "\n",
    "        # Add U-Net\n",
    "        self.unet = U_net(self.in_channels, self.out_channels, unet_kernel_size, unet_dropout_rate)\n",
    "\n",
    "        # Each block will have 2 norms if we also use an MLP\n",
    "        self.n_norms = 1 if self.mlp is None else 2\n",
    "        if norm is None:\n",
    "            self.norm = None\n",
    "        elif norm == \"instance_norm\":\n",
    "            self.norm = nn.ModuleList(\n",
    "                [\n",
    "                    getattr(nn, f\"InstanceNorm{self.n_dim}d\")(\n",
    "                        num_features=self.out_channels\n",
    "                    )\n",
    "                    for _ in range(n_layers * self.n_norms)\n",
    "                ]\n",
    "            )\n",
    "        elif norm == \"group_norm\":\n",
    "            self.norm = nn.ModuleList(\n",
    "                [\n",
    "                    nn.GroupNorm(num_groups=1, num_channels=self.out_channels)\n",
    "                    for _ in range(n_layers * self.n_norms)\n",
    "                ]\n",
    "            )\n",
    "        elif norm == \"ada_in\":\n",
    "            self.norm = nn.ModuleList(\n",
    "                [\n",
    "                    AdaIN(ada_in_features, out_channels)\n",
    "                    for _ in range(n_layers * self.n_norms)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Got norm={norm} but expected None or one of \"\n",
    "                \"[instance_norm, group_norm, ada_in]\"\n",
    "            )\n",
    "\n",
    "    def forward(self, x, index=0, output_shape=None):\n",
    "        if self.preactivation:\n",
    "            return self.forward_with_preactivation(x, index, output_shape)\n",
    "        else:\n",
    "            return self.forward_with_postactivation(x, index, output_shape)\n",
    "\n",
    "    def forward_with_postactivation(self, x, index=0, output_shape=None):\n",
    "        x_skip_fno = self.fno_skips[index](x)\n",
    "        x_skip_fno = self.convs[index].transform(x_skip_fno, output_shape=output_shape)\n",
    "\n",
    "        if self.mlp is not None:\n",
    "            x_skip_mlp = self.mlp_skips[index](x)\n",
    "            x_skip_mlp = self.convs[index].transform(x_skip_mlp, output_shape=output_shape)\n",
    "\n",
    "        if self.stabilizer == \"tanh\":\n",
    "            x = torch.tanh(x)\n",
    "\n",
    "        x_fno = self.convs(x, index, output_shape=output_shape)\n",
    "        x_unet = self.unet(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x_fno = self.norm[self.n_norms * index](x_fno)\n",
    "\n",
    "        x = x_fno + x_unet + x_skip_fno\n",
    "\n",
    "        if (self.mlp is not None) or (index < (self.n_layers - 1)):\n",
    "            x = self.non_linearity(x)\n",
    "\n",
    "        if self.mlp is not None:\n",
    "            x = self.mlp[index](x) + x_skip_mlp\n",
    "\n",
    "            if self.norm is not None:\n",
    "                x = self.norm[self.n_norms * index + 1](x)\n",
    "\n",
    "            if index < (self.n_layers - 1):\n",
    "                x = self.non_linearity(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_with_preactivation(self, x, index=0, output_shape=None):\n",
    "        x = self.non_linearity(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm[self.n_norms * index](x)\n",
    "\n",
    "        x_skip_fno = self.fno_skips[index](x)\n",
    "        x_skip_fno = self.convs[index].transform(x_skip_fno, output_shape=output_shape)\n",
    "\n",
    "        if self.mlp is not None:\n",
    "            x_skip_mlp = self.mlp_skips[index](x)\n",
    "            x_skip_mlp = self.convs[index].transform(x_skip_mlp, output_shape=output_shape)\n",
    "\n",
    "        if self.stabilizer == \"tanh\":\n",
    "            x = torch.tanh(x)\n",
    "\n",
    "        x_fno = self.convs(x, index, output_shape=output_shape)\n",
    "        x_unet = self.unet(x)\n",
    "        x = x_fno + x_unet + x_skip_fno\n",
    "\n",
    "        if self.mlp is not None:\n",
    "            if index < (self.n_layers - 1):\n",
    "                x = self.non_linearity(x)\n",
    "\n",
    "            if self.norm is not None:\n",
    "                x = self.norm[self.n_norms * index + 1](x)\n",
    "\n",
    "            x = self.mlp[index](x) + x_skip_mlp\n",
    "\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def n_modes(self):\n",
    "        return self._n_modes\n",
    "\n",
    "    @n_modes.setter\n",
    "    def n_modes(self, n_modes):\n",
    "        self.convs.n_modes = n_modes\n",
    "        self._n_modes = n_modes\n",
    "\n",
    "    def get_block(self, indices):\n",
    "        if self.n_layers == 1:\n",
    "            raise ValueError(\n",
    "                \"A single layer is parametrized, directly use the main class.\"\n",
    "            )\n",
    "        return SubModule(self, indices)\n",
    "\n",
    "    def __getitem__(self, indices):\n",
    "        return self.get_block(indices)\n",
    "\n",
    "class SubModule(nn.Module):\n",
    "    def __init__(self, main_module, indices):\n",
    "        super().__init__()\n",
    "        self.main_module = main_module\n",
    "        self.indices = indices\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main_module.forward(x, self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2d5d21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralop.layers.spectral_convolution import SpectralConv\n",
    "from neuralop.layers.padding import DomainPadding\n",
    "from neuralop.layers.fno_block import FNOBlocks\n",
    "from neuralop.layers.mlp import MLP\n",
    "from neuralop.models.base_model import BaseModel\n",
    "\n",
    "class CustomFNO(BaseModel, name='CustomFNO'):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_modes,\n",
    "        hidden_channels,\n",
    "        in_channels=3,\n",
    "        out_channels=1,\n",
    "        lifting_channels=256,\n",
    "        projection_channels=256,\n",
    "        n_layers=4,\n",
    "        output_scaling_factor=None,\n",
    "        max_n_modes=None,\n",
    "        fno_block_precision=\"full\",\n",
    "        use_mlp=False,\n",
    "        mlp_dropout=0,\n",
    "        mlp_expansion=0.5,\n",
    "        non_linearity=F.gelu,\n",
    "        stabilizer=None,\n",
    "        norm=None,\n",
    "        preactivation=False,\n",
    "        fno_skip=\"linear\",\n",
    "        mlp_skip=\"soft-gating\",\n",
    "        separable=False,\n",
    "        factorization=None,\n",
    "        rank=1.0,\n",
    "        joint_factorization=False,\n",
    "        fixed_rank_modes=False,\n",
    "        implementation=\"factorized\",\n",
    "        decomposition_kwargs=dict(),\n",
    "        domain_padding=None,\n",
    "        domain_padding_mode=\"one-sided\",\n",
    "        fft_norm=\"forward\",\n",
    "        SpectralConv=SpectralConv,\n",
    "        unet_kernel_size=3,\n",
    "        unet_dropout_rate=0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_dim = len(n_modes)\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Define the lifting layers\n",
    "        self.lifting1 = MLP(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            hidden_channels=lifting_channels,\n",
    "            n_layers=2,\n",
    "            n_dim=self.n_dim,\n",
    "        )\n",
    "\n",
    "        # Define the FNO blocks\n",
    "        self.fno_blocks = FNOBlocks(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            n_modes=n_modes,\n",
    "            output_scaling_factor=output_scaling_factor,\n",
    "            use_mlp=use_mlp,\n",
    "            mlp_dropout=mlp_dropout,\n",
    "            mlp_expansion=mlp_expansion,\n",
    "            non_linearity=non_linearity,\n",
    "            stabilizer=stabilizer,\n",
    "            norm=norm,\n",
    "            preactivation=preactivation,\n",
    "            fno_skip=fno_skip,\n",
    "            mlp_skip=mlp_skip,\n",
    "            max_n_modes=max_n_modes,\n",
    "            fno_block_precision=fno_block_precision,\n",
    "            rank=rank,\n",
    "            fft_norm=fft_norm,\n",
    "            fixed_rank_modes=fixed_rank_modes,\n",
    "            implementation=implementation,\n",
    "            separable=separable,\n",
    "            factorization=factorization,\n",
    "            decomposition_kwargs=decomposition_kwargs,\n",
    "            joint_factorization=joint_factorization,\n",
    "            SpectralConv=SpectralConv,\n",
    "            n_layers=n_layers // 2,  # Use half the layers for FNOBlocks\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        self.fno_blocks_unet = FNOBlockswithUnet(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            n_modes=n_modes,\n",
    "            output_scaling_factor=output_scaling_factor,\n",
    "            use_mlp=use_mlp,\n",
    "            mlp_dropout=mlp_dropout,\n",
    "            mlp_expansion=mlp_expansion,\n",
    "            non_linearity=non_linearity,\n",
    "            stabilizer=stabilizer,\n",
    "            norm=norm,\n",
    "            preactivation=preactivation,\n",
    "            fno_skip=fno_skip,\n",
    "            mlp_skip=mlp_skip,\n",
    "            max_n_modes=max_n_modes,\n",
    "            fno_block_precision=fno_block_precision,\n",
    "            rank=rank,\n",
    "            fft_norm=fft_norm,\n",
    "            fixed_rank_modes=fixed_rank_modes,\n",
    "            implementation=implementation,\n",
    "            separable=separable,\n",
    "            factorization=factorization,\n",
    "            decomposition_kwargs=decomposition_kwargs,\n",
    "            joint_factorization=joint_factorization,\n",
    "            SpectralConv=SpectralConv,\n",
    "            n_layers=n_layers - (n_layers // 2),  # Use remaining layers for FNOBlockswithUnet\n",
    "            unet_kernel_size=unet_kernel_size,\n",
    "            unet_dropout_rate=unet_dropout_rate,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Define the projection layer\n",
    "        self.projection = MLP(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=out_channels,\n",
    "            hidden_channels=projection_channels,\n",
    "            n_layers=2,\n",
    "            n_dim=self.n_dim,\n",
    "            non_linearity=non_linearity,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for the Custom FNO model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : tensor\n",
    "            Input tensor of shape [batch_size, in_channels, 504, 504]\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.lifting1(x)  # Result shape: [batch_size, hidden_channels, 504, 504]\n",
    "\n",
    "        # Apply FNO blocks for the first half of layers\n",
    "        for layer_idx in range(self.n_layers // 2):\n",
    "            x = self.fno_blocks(x, layer_idx)\n",
    "            \n",
    "        # Apply FNOBlockswithUnet for the second half of layers\n",
    "        for layer_idx in range(self.n_layers - (self.n_layers // 2)):\n",
    "            x = self.fno_blocks_unet(x, layer_idx)\n",
    "\n",
    "        x = self.projection(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0efc3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, hdf5_file_path):\n",
    "        self.hdf5_file_path = hdf5_file_path\n",
    "        self.hdf5_file = h5py.File(hdf5_file_path, 'r')\n",
    "        self.dataset_length = len(self.hdf5_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_data = torch.from_numpy(self.hdf5_file[f'sample_{idx}/input'][:]).float()\n",
    "        output_data = torch.from_numpy(self.hdf5_file[f'sample_{idx}/output'][:]).float()\n",
    "        \n",
    "        input_data = input_data.unsqueeze(0)  \n",
    "        \n",
    "        sample = {\n",
    "            'input':  input_data,\n",
    "            'output': output_data\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "processed_data_path = \"/data.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4044f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed_value = 1\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Create the custom dataset\n",
    "custom_dataset = CustomDataset(hdf5_file_path=processed_data_path)\n",
    "\n",
    "# Define the size for the test set\n",
    "test_set_size = 20\n",
    "\n",
    "# Generate random indices for the test set and corresponding training set\n",
    "all_indices = list(range(len(custom_dataset)))\n",
    "random.shuffle(all_indices)\n",
    "\n",
    "test_indices = all_indices[:test_set_size]\n",
    "train_indices = all_indices[test_set_size:]\n",
    "\n",
    "# Create training dataset using Subset\n",
    "train_dataset = Subset(custom_dataset, train_indices)\n",
    "\n",
    "# Create testing dataset using Subset\n",
    "test_dataset = Subset(custom_dataset, test_indices)\n",
    "\n",
    "# Example usage in a DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0, persistent_workers=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=0, persistent_workers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f77977",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch['input'].shape)\n",
    "    print(batch['output'].shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950819bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomFNO(n_modes=(64,64), in_channels=1, out_channels=99, hidden_channels=128, use_mlp=True, n_layers=4,\n",
    "                              factorization='tucker', rank=0.5)\n",
    "\n",
    "model = model.to(device)\n",
    "n_params = count_model_params(model)\n",
    "print(f'\\nYour model has {n_params} parameters.')\n",
    "sys.stdout.flush()  # flush the stdout buffer\n",
    "\n",
    "h1loss = H1Loss(d=2)\n",
    "test_loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c07d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e025f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "\n",
    "\n",
    "num_epochs = 300\n",
    "print_frequency = 40\n",
    "save_checkpoint_start_epoch = 100\n",
    "save_checkpoint_interval = 50\n",
    "checkpoint_dir = \"/result/\"\n",
    "results_file = os.path.join(checkpoint_dir, \"result.csv\")\n",
    "\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "test_r2_losses = []\n",
    "test_mse_losses = []\n",
    "epoch_durations = []\n",
    "\n",
    "results_file = os.path.join(checkpoint_dir, \"results.csv\")\n",
    "with open(results_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['epoch', 'r2', 'relative_error', 'mse', 'mae', 'psnr', 'ssim'])\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    train_loader_tqdm = tqdm(train_loader, total=len(train_loader), desc=f'Train Epoch {epoch+1}/{num_epochs}')\n",
    "    for batch_idx, batch in enumerate(train_loader_tqdm):\n",
    "        optimizer.zero_grad()\n",
    "        data, target = batch['input'].to(device), batch['output'].to(device)\n",
    "\n",
    "        output = model(data) \n",
    "\n",
    "        loss = h1loss(output, target).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if batch_idx % print_frequency == 0:\n",
    "            train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    test_loader_tqdm = tqdm(test_loader, total=len(test_loader), desc=f'Test Epoch {epoch+1}/{num_epochs}')\n",
    "    r2_scores = []\n",
    "    mse_losses = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader_tqdm:\n",
    "            data, target = batch['input'].to(device), batch['output'].to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            output_np = output.cpu().numpy()\n",
    "            target_np = target.cpu().numpy()\n",
    "            \n",
    "            for true_sample, pred_sample in zip(target_np, output_np):\n",
    "                r2 = r2_score(true_sample.flatten(), pred_sample.flatten())\n",
    "                r2_scores.append(r2)\n",
    "            \n",
    "            mse_loss = test_loss(output, target).item()\n",
    "            mse_losses.append(mse_loss)\n",
    "            \n",
    "\n",
    "\n",
    "    avg_r2 = np.mean(r2_scores)\n",
    "    avg_mse = np.mean(mse_losses)\n",
    "\n",
    "    test_r2_losses.append(avg_r2)\n",
    "    test_mse_losses.append(avg_mse)\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step(avg_train_loss)\n",
    "\n",
    "    # End of timer\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    epoch_durations.append(epoch_duration)\n",
    "\n",
    "    # Calculate remaining time\n",
    "    avg_epoch_duration = np.mean(epoch_durations)\n",
    "    remaining_epochs = num_epochs - (epoch + 1)\n",
    "    remaining_time = remaining_epochs * avg_epoch_duration\n",
    "    hours, rem = divmod(remaining_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Duration: {epoch_duration:.2f}s, '\n",
    "          f'Train Loss: {avg_train_loss:.4f}, Test R2: {avg_r2:.4f}, Test MSE: {avg_mse:.4f}')\n",
    "    print(f'Estimated Remaining Time: {int(hours)}h {int(minutes)}m {int(seconds)}s')\n",
    "\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if (epoch + 1) >= save_checkpoint_start_epoch and (epoch + 1) % save_checkpoint_interval == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved at epoch {epoch+1}: {checkpoint_path}\")\n",
    "\n",
    "        model.eval()\n",
    "        sample_r2_scores, sample_relative_errors, sample_mses, sample_maes, sample_psnr_values, sample_ssim_values = [], [], [], [], [], []\n",
    "        \n",
    "        min_val = float('inf')\n",
    "        max_val = float('-inf')\n",
    "\n",
    "        for sample in test_loader:\n",
    "            y_true_batch = sample['output'].cpu().numpy()\n",
    "            batch_min = y_true_batch.min()\n",
    "            batch_max = y_true_batch.max()\n",
    "            min_val = min(min_val, batch_min)\n",
    "            max_val = max(max_val, batch_max)\n",
    "\n",
    "        data_range = max_val - min_val\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sample in test_loader:\n",
    "                x = sample['input'].to(device)\n",
    "                y_true_batch = sample['output'].cpu().numpy()\n",
    "                y_pred_batch = model(x).cpu().numpy()\n",
    "\n",
    "                for true_sample, pred_sample in zip(y_true_batch, y_pred_batch):\n",
    "                    true_sample_component = true_sample\n",
    "                    pred_sample_component = pred_sample\n",
    "                    \n",
    "                    true_sample_flat = true_sample.flatten()\n",
    "                    pred_sample_flat = pred_sample.flatten()\n",
    "\n",
    "                    # R-squared\n",
    "                    r2 = r2_score(true_sample_flat, pred_sample_flat)\n",
    "                    sample_r2_scores.append(r2)\n",
    "\n",
    "                    # Relative error\n",
    "                    absolute_error = np.linalg.norm(true_sample_flat - pred_sample_flat, 2)\n",
    "                    relative_error = absolute_error / np.linalg.norm(true_sample_flat, 2)\n",
    "                    sample_relative_errors.append(relative_error)\n",
    "\n",
    "                    # MSE\n",
    "                    mse = mean_squared_error(true_sample_flat, pred_sample_flat)\n",
    "                    sample_mses.append(mse)\n",
    "\n",
    "                    # MAE\n",
    "                    mae = mean_absolute_error(true_sample_flat, pred_sample_flat)\n",
    "                    sample_maes.append(mae)\n",
    "                    \n",
    "                    # PSNR \n",
    "                    psnr_value = psnr(true_sample_component, pred_sample_component, data_range=data_range)\n",
    "                    sample_psnr_values.append(psnr_value)\n",
    "\n",
    "                    # SSIM \n",
    "                    ssim_value = ssim(true_sample_component, pred_sample_component, data_range=data_range)\n",
    "                    sample_ssim_values.append(ssim_value)\n",
    "\n",
    "            avg_r2 = np.mean(sample_r2_scores)\n",
    "            avg_relative_error = np.mean(sample_relative_errors)\n",
    "            avg_mse = np.mean(sample_mses)\n",
    "            avg_mae = np.mean(sample_maes)\n",
    "            avg_psnr = np.mean(sample_psnr_values)\n",
    "            avg_ssim = np.mean(sample_ssim_values)\n",
    "\n",
    "        with open(results_file, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch+1, avg_r2, avg_relative_error, avg_mse, avg_mae, avg_psnr, avg_ssim])\n",
    "\n",
    "# Save final losses\n",
    "np.save(os.path.join(checkpoint_dir, 'train_losses.npy'), np.array(train_losses))\n",
    "np.save(os.path.join(checkpoint_dir, 'test_losses.npy'), np.array(test_r2_losses))\n",
    "np.save(os.path.join(checkpoint_dir, 'test_mse_losses.npy'), np.array(test_mse_losses))\n",
    "\n",
    "# Average Epoch Duration\n",
    "average_epoch_duration = np.mean(epoch_durations)\n",
    "print(f'Average Epoch Duration: {average_epoch_duration:.2f}s')\n",
    "\n",
    "# Total Training Time\n",
    "total_training_time = np.sum(epoch_durations)\n",
    "print(f'Total Training Time: {total_training_time:.2f}s')\n",
    "\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Test R2 Score\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(test_r2_losses, label='Test R2 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('R2 Score')\n",
    "plt.title('Test R2 Score Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Test MSE Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(test_mse_losses, label='Test MSE Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Test MSE Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

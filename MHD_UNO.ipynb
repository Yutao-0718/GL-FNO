{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9dabf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from neuralop.models import UNO\n",
    "from neuralop.training.trainer import Trainer\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop.losses.data_losses import LpLoss, H1Loss\n",
    "import pdb\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from torch.utils.data.dataset import Subset\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.transforms.functional import normalize\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "import h5py\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724a1024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, hdf5_file_path):\n",
    "        self.hdf5_file_path = hdf5_file_path\n",
    "        self.hdf5_file = h5py.File(hdf5_file_path, 'r')\n",
    "        self.dataset_length = len(self.hdf5_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_data = torch.from_numpy(self.hdf5_file[f'sample_{idx}/input'][:]).float()\n",
    "        output_data = torch.from_numpy(self.hdf5_file[f'sample_{idx}/output'][:]).float()\n",
    "        \n",
    "        input_data = input_data.unsqueeze(0)  \n",
    "        \n",
    "        sample = {\n",
    "            'input':  input_data,\n",
    "            'output': output_data\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "processed_data_path = \"/data.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf62ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed_value = 1\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Create the custom dataset\n",
    "custom_dataset = CustomDataset(hdf5_file_path=processed_data_path)\n",
    "\n",
    "# Define the size for the test set\n",
    "test_set_size = 20\n",
    "\n",
    "# Generate random indices for the test set and corresponding training set\n",
    "all_indices = list(range(len(custom_dataset)))\n",
    "random.shuffle(all_indices)\n",
    "\n",
    "test_indices = all_indices[:test_set_size]\n",
    "train_indices = all_indices[test_set_size:]\n",
    "\n",
    "# Create training dataset using Subset\n",
    "train_dataset = Subset(custom_dataset, train_indices)\n",
    "\n",
    "# Create testing dataset using Subset\n",
    "test_dataset = Subset(custom_dataset, test_indices)\n",
    "\n",
    "# Example usage in a DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0, persistent_workers=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=0, persistent_workers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNO(in_channels=1,\n",
    "    out_channels=99,\n",
    "    hidden_channels=128,\n",
    "    uno_out_channels=[32, 64, 64, 32],\n",
    "    uno_n_modes= [[64,64],[64,64],[64,64],[64,64]], \n",
    "    uno_scalings=  [[1.0,1.0],[0.5,0.5],[1,1],[2,2]],\n",
    "    use_mlp=True,\n",
    "    preactivation=True,\n",
    "    factorization=\"tucker\",\n",
    "    rank=0.5,\n",
    "    mlp_skip='linear'\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "n_params = count_model_params(model)\n",
    "print(f'\\nYour model has {n_params} parameters.')\n",
    "sys.stdout.flush()  # flush the stdout buffer\n",
    "\n",
    "h1loss = H1Loss(d=2)\n",
    "test_loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-4) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d5e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "\n",
    "num_epochs = 300\n",
    "print_frequency = 40\n",
    "save_checkpoint_start_epoch = 100\n",
    "save_checkpoint_interval = 50\n",
    "checkpoint_dir = \"/result/\"\n",
    "results_file = os.path.join(checkpoint_dir, \"result.csv\")\n",
    "\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "test_r2_losses = []\n",
    "test_mse_losses = []\n",
    "epoch_durations = []\n",
    "\n",
    "results_file = os.path.join(checkpoint_dir, \"results.csv\")\n",
    "with open(results_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['epoch', 'r2', 'relative_error', 'mse', 'mae', 'psnr', 'ssim'])\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    train_loader_tqdm = tqdm(train_loader, total=len(train_loader), desc=f'Train Epoch {epoch+1}/{num_epochs}')\n",
    "    for batch_idx, batch in enumerate(train_loader_tqdm):\n",
    "        optimizer.zero_grad()\n",
    "        data, target = batch['input'].to(device), batch['output'].to(device)\n",
    "\n",
    "        output = model(data) \n",
    "\n",
    "        loss = h1loss(output, target).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if batch_idx % print_frequency == 0:\n",
    "            train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    test_loader_tqdm = tqdm(test_loader, total=len(test_loader), desc=f'Test Epoch {epoch+1}/{num_epochs}')\n",
    "    r2_scores = []\n",
    "    mse_losses = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader_tqdm:\n",
    "            data, target = batch['input'].to(device), batch['output'].to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            output_np = output.cpu().numpy()\n",
    "            target_np = target.cpu().numpy()\n",
    "            \n",
    "            for true_sample, pred_sample in zip(target_np, output_np):\n",
    "                r2 = r2_score(true_sample.flatten(), pred_sample.flatten())\n",
    "                r2_scores.append(r2)\n",
    "            \n",
    "            mse_loss = test_loss(output, target).item()\n",
    "            mse_losses.append(mse_loss)\n",
    "            \n",
    "\n",
    "\n",
    "    avg_r2 = np.mean(r2_scores)\n",
    "    avg_mse = np.mean(mse_losses)\n",
    "\n",
    "    test_r2_losses.append(avg_r2)\n",
    "    test_mse_losses.append(avg_mse)\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step(avg_train_loss)\n",
    "\n",
    "    # End of timer\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    epoch_durations.append(epoch_duration)\n",
    "\n",
    "    # Calculate remaining time\n",
    "    avg_epoch_duration = np.mean(epoch_durations)\n",
    "    remaining_epochs = num_epochs - (epoch + 1)\n",
    "    remaining_time = remaining_epochs * avg_epoch_duration\n",
    "    hours, rem = divmod(remaining_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Duration: {epoch_duration:.2f}s, '\n",
    "          f'Train Loss: {avg_train_loss:.4f}, Test R2: {avg_r2:.4f}, Test MSE: {avg_mse:.4f}')\n",
    "    print(f'Estimated Remaining Time: {int(hours)}h {int(minutes)}m {int(seconds)}s')\n",
    "\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if (epoch + 1) >= save_checkpoint_start_epoch and (epoch + 1) % save_checkpoint_interval == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved at epoch {epoch+1}: {checkpoint_path}\")\n",
    "\n",
    "        model.eval()\n",
    "        sample_r2_scores, sample_relative_errors, sample_mses, sample_maes, sample_psnr_values, sample_ssim_values = [], [], [], [], [], []\n",
    "        \n",
    "        min_val = float('inf')\n",
    "        max_val = float('-inf')\n",
    "\n",
    "        for sample in test_loader:\n",
    "            y_true_batch = sample['output'].cpu().numpy()\n",
    "            batch_min = y_true_batch.min()\n",
    "            batch_max = y_true_batch.max()\n",
    "            min_val = min(min_val, batch_min)\n",
    "            max_val = max(max_val, batch_max)\n",
    "\n",
    "        data_range = max_val - min_val\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sample in test_loader:\n",
    "                x = sample['input'].to(device)\n",
    "                y_true_batch = sample['output'].cpu().numpy()\n",
    "                y_pred_batch = model(x).cpu().numpy()\n",
    "\n",
    "                for true_sample, pred_sample in zip(y_true_batch, y_pred_batch):\n",
    "                    true_sample_component = true_sample\n",
    "                    pred_sample_component = pred_sample\n",
    "                    \n",
    "                    true_sample_flat = true_sample.flatten()\n",
    "                    pred_sample_flat = pred_sample.flatten()\n",
    "\n",
    "                    # R-squared\n",
    "                    r2 = r2_score(true_sample_flat, pred_sample_flat)\n",
    "                    sample_r2_scores.append(r2)\n",
    "\n",
    "                    # Relative error\n",
    "                    absolute_error = np.linalg.norm(true_sample_flat - pred_sample_flat, 2)\n",
    "                    relative_error = absolute_error / np.linalg.norm(true_sample_flat, 2)\n",
    "                    sample_relative_errors.append(relative_error)\n",
    "\n",
    "                    # MSE\n",
    "                    mse = mean_squared_error(true_sample_flat, pred_sample_flat)\n",
    "                    sample_mses.append(mse)\n",
    "\n",
    "                    # MAE\n",
    "                    mae = mean_absolute_error(true_sample_flat, pred_sample_flat)\n",
    "                    sample_maes.append(mae)\n",
    "                    \n",
    "                    # PSNR \n",
    "                    psnr_value = psnr(true_sample_component, pred_sample_component, data_range=data_range)\n",
    "                    sample_psnr_values.append(psnr_value)\n",
    "\n",
    "                    # SSIM \n",
    "                    ssim_value = ssim(true_sample_component, pred_sample_component, data_range=data_range)\n",
    "                    sample_ssim_values.append(ssim_value)\n",
    "\n",
    "            avg_r2 = np.mean(sample_r2_scores)\n",
    "            avg_relative_error = np.mean(sample_relative_errors)\n",
    "            avg_mse = np.mean(sample_mses)\n",
    "            avg_mae = np.mean(sample_maes)\n",
    "            avg_psnr = np.mean(sample_psnr_values)\n",
    "            avg_ssim = np.mean(sample_ssim_values)\n",
    "\n",
    "        with open(results_file, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch+1, avg_r2, avg_relative_error, avg_mse, avg_mae, avg_psnr, avg_ssim])\n",
    "\n",
    "# Save final losses\n",
    "np.save(os.path.join(checkpoint_dir, 'train_losses.npy'), np.array(train_losses))\n",
    "np.save(os.path.join(checkpoint_dir, 'test_r2_losses.npy'), np.array(test_r2_losses))\n",
    "np.save(os.path.join(checkpoint_dir, 'test_mse_losses.npy'), np.array(test_mse_losses))\n",
    "\n",
    "# Average Epoch Duration\n",
    "average_epoch_duration = np.mean(epoch_durations)\n",
    "print(f'Average Epoch Duration: {average_epoch_duration:.2f}s')\n",
    "\n",
    "# Total Training Time\n",
    "total_training_time = np.sum(epoch_durations)\n",
    "print(f'Total Training Time: {total_training_time:.2f}s')\n",
    "\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Test R2 Score\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(test_r2_losses, label='Test R2 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('R2 Score')\n",
    "plt.title('Test R2 Score Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Test MSE Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(test_mse_losses, label='Test MSE Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Test MSE Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739dfaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

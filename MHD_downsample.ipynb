{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb661a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partialmethod\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from neuralop.layers.spectral_convolution import SpectralConv\n",
    "from neuralop.layers.padding import DomainPadding\n",
    "from neuralop.layers.fno_block import FNOBlocks\n",
    "from neuralop.layers.mlp import MLP\n",
    "from neuralop.models.base_model import BaseModel\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from neuralop.models.fno import TFNO\n",
    "from neuralop.models.fno import TFNO3d\n",
    "from neuralop.training.trainer import Trainer\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop.losses.data_losses import LpLoss, H1Loss\n",
    "import pdb\n",
    "import sys\n",
    "from torch.utils.data.dataset import Subset\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.transforms.functional import normalize\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb05adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, hdf5_file_path):\n",
    "        self.hdf5_file_path = hdf5_file_path\n",
    "        self.hdf5_file = h5py.File(hdf5_file_path, 'r')\n",
    "        self.dataset_length = len(self.hdf5_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_data = torch.from_numpy(self.hdf5_file[f'sample_{idx}/input'][:]).float()\n",
    "        output_data = torch.from_numpy(self.hdf5_file[f'sample_{idx}/output'][:]).float()\n",
    "        \n",
    "        input_data = input_data.unsqueeze(0).unsqueeze(0)  \n",
    "        output_data = output_data.unsqueeze(0)  \n",
    "        \n",
    "        \n",
    "        input_data_downsampled = F.interpolate(input_data, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
    "        output_data_downsampled = F.interpolate(output_data, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        input_data_downsampled = input_data_downsampled.squeeze(0)  \n",
    "        output_data_downsampled = output_data_downsampled.squeeze(0)  \n",
    "        \n",
    "        sample = {\n",
    "            'input':  input_data_downsampled,\n",
    "            'output': output_data_downsampled\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "processed_data_path = \"/data.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f02c0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed_value = 1\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Create the custom dataset\n",
    "custom_dataset = CustomDataset(hdf5_file_path=processed_data_path)\n",
    "\n",
    "# Define the size for the test set\n",
    "test_set_size = 20\n",
    "\n",
    "# Generate random indices for the test set and corresponding training set\n",
    "all_indices = list(range(len(custom_dataset)))\n",
    "random.shuffle(all_indices)\n",
    "\n",
    "test_indices = all_indices[:test_set_size]\n",
    "train_indices = all_indices[test_set_size:]\n",
    "\n",
    "# Create training dataset using Subset\n",
    "train_dataset = Subset(custom_dataset, train_indices)\n",
    "\n",
    "# Create testing dataset using Subset\n",
    "test_dataset = Subset(custom_dataset, test_indices)\n",
    "\n",
    "# Example usage in a DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0, persistent_workers=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=0, persistent_workers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    x_shape = batch['input'].shape\n",
    "    y_shape = batch['output'].shape\n",
    "    print(f'Batch X shape: {x_shape}')\n",
    "    print(f'Batch Y shape: {y_shape}')\n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6222bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFNO(BaseModel, name='CustomFNO'):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_modes,\n",
    "        hidden_channels,\n",
    "        in_channels=3,\n",
    "        out_channels=1,\n",
    "        lifting_channels=256,\n",
    "        projection_channels=256,\n",
    "        n_layers=4,\n",
    "        output_scaling_factor=None,\n",
    "        max_n_modes=None,\n",
    "        fno_block_precision=\"full\",\n",
    "        use_mlp=False,\n",
    "        mlp_dropout=0,\n",
    "        mlp_expansion=0.5,\n",
    "        non_linearity=F.gelu,\n",
    "        stabilizer=None,\n",
    "        norm=None,\n",
    "        preactivation=False,\n",
    "        fno_skip=\"linear\",\n",
    "        mlp_skip=\"soft-gating\",\n",
    "        separable=False,\n",
    "        factorization=None,\n",
    "        rank=1.0,\n",
    "        joint_factorization=False,\n",
    "        fixed_rank_modes=False,\n",
    "        implementation=\"factorized\",\n",
    "        decomposition_kwargs=dict(),\n",
    "        domain_padding=None,\n",
    "        domain_padding_mode=\"one-sided\",\n",
    "        fft_norm=\"forward\",\n",
    "        SpectralConv=SpectralConv,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_dim = len(n_modes)\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Define the lifting layers\n",
    "        self.lifting1 = MLP(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            hidden_channels=lifting_channels,\n",
    "            n_layers=2,\n",
    "            n_dim=self.n_dim,\n",
    "        )\n",
    "\n",
    "        # Define the FNO blocks\n",
    "        self.fno_blocks = FNOBlocks(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            n_modes=n_modes,\n",
    "            output_scaling_factor=output_scaling_factor,\n",
    "            use_mlp=use_mlp,\n",
    "            mlp_dropout=mlp_dropout,\n",
    "            mlp_expansion=mlp_expansion,\n",
    "            non_linearity=non_linearity,\n",
    "            stabilizer=stabilizer,\n",
    "            norm=norm,\n",
    "            preactivation=preactivation,\n",
    "            fno_skip=fno_skip,\n",
    "            mlp_skip=mlp_skip,\n",
    "            max_n_modes=max_n_modes,\n",
    "            fno_block_precision=fno_block_precision,\n",
    "            rank=rank,\n",
    "            fft_norm=fft_norm,\n",
    "            fixed_rank_modes=fixed_rank_modes,\n",
    "            implementation=implementation,\n",
    "            separable=separable,\n",
    "            factorization=factorization,\n",
    "            decomposition_kwargs=decomposition_kwargs,\n",
    "            joint_factorization=joint_factorization,\n",
    "            SpectralConv=SpectralConv,\n",
    "            n_layers=n_layers,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Define the projection layer\n",
    "        self.projection = MLP(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=out_channels,\n",
    "            hidden_channels=projection_channels,\n",
    "            n_layers=2,\n",
    "            n_dim=self.n_dim,\n",
    "            non_linearity=non_linearity,\n",
    "        )\n",
    "\n",
    "    def forward(self, x1):\n",
    "        \"\"\"Forward pass for the Custom FNO model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x1 : tensor\n",
    "            Input tensor of shape [batch_size, 1, 504, 504]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Path 2: Downsample, process, then upsample\n",
    "        x1_downsampled = self.lifting1(x1)  # Apply lifting\n",
    "\n",
    "        for layer_idx in range(self.n_layers):\n",
    "            x1_downsampled = self.fno_blocks(x1_downsampled, layer_idx)\n",
    "\n",
    "        x = self.projection(x1_downsampled)\n",
    "\n",
    "\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517fee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomFNO(n_modes=(64,64), in_channels=1, out_channels=99, hidden_channels=128, projection_channels=256, use_mlp=True,\n",
    "                              factorization='tucker', rank=0.5)\n",
    "model = model.to(device)\n",
    "\n",
    "n_params = count_model_params(model)\n",
    "print(f'\\nYour model has {n_params} parameters.')\n",
    "sys.stdout.flush()  # flush the stdout buffer\n",
    "h1loss = H1Loss(d=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-4) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5cb0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "\n",
    "num_epochs = 300\n",
    "print_frequency = 40\n",
    "save_checkpoint_start_epoch = 100\n",
    "save_checkpoint_interval = 50\n",
    "checkpoint_dir = \"/result/\"\n",
    "results_file = os.path.join(checkpoint_dir, \"result.csv\")\n",
    "\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "epoch_durations = []\n",
    "\n",
    "results_file = os.path.join(checkpoint_dir, \"results.csv\")\n",
    "with open(results_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['epoch', 'r2', 'relative_error', 'mse', 'mae', 'psnr', 'ssim'])\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    train_loader_tqdm = tqdm(train_loader, total=len(train_loader), desc=f'Train Epoch {epoch+1}/{num_epochs}')\n",
    "    for batch_idx, batch in enumerate(train_loader_tqdm):\n",
    "        optimizer.zero_grad()\n",
    "        data, target = batch['input'].to(device), batch['output'].to(device)\n",
    "\n",
    "        output = model(data) \n",
    "\n",
    "        loss = h1loss(output, target).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if batch_idx % print_frequency == 0:\n",
    "            train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    test_loader_tqdm = tqdm(test_loader, total=len(test_loader), desc=f'Test Epoch {epoch+1}/{num_epochs}')\n",
    "    r2_scores = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader_tqdm:\n",
    "            data, target = batch['input'].to(device), batch['output'].to(device)\n",
    "            output = model(data).cpu().numpy() \n",
    "            \n",
    "            for true_sample, pred_sample in zip(target.cpu().numpy(), output):\n",
    "                r2 = r2_score(true_sample.flatten(), pred_sample.flatten())\n",
    "                r2_scores.append(r2)\n",
    "\n",
    "\n",
    "    avg_r2 = np.mean(r2_scores)\n",
    "\n",
    "    test_losses.append(avg_r2)\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step(avg_train_loss)\n",
    "\n",
    "    # End of timer\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    epoch_durations.append(epoch_duration)\n",
    "\n",
    "    # Calculate remaining time\n",
    "    avg_epoch_duration = np.mean(epoch_durations)\n",
    "    remaining_epochs = num_epochs - (epoch + 1)\n",
    "    remaining_time = remaining_epochs * avg_epoch_duration\n",
    "    hours, rem = divmod(remaining_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Duration: {epoch_duration:.2f}s, '\n",
    "          f'Train Loss: {avg_train_loss:.4f}, Test R2: {avg_r2:.4f}')\n",
    "    print(f'Estimated Remaining Time: {int(hours)}h {int(minutes)}m {int(seconds)}s')\n",
    "\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if (epoch + 1) >= save_checkpoint_start_epoch and (epoch + 1) % save_checkpoint_interval == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved at epoch {epoch+1}: {checkpoint_path}\")\n",
    "\n",
    "        model.eval()\n",
    "        sample_r2_scores, sample_relative_errors, sample_mses, sample_maes, sample_psnr_values, sample_ssim_values = [], [], [], [], [], []\n",
    "        \n",
    "        min_val = float('inf')\n",
    "        max_val = float('-inf')\n",
    "\n",
    "        for sample in test_loader:\n",
    "            y_true_batch = sample['output'].cpu().numpy()\n",
    "            batch_min = y_true_batch.min()\n",
    "            batch_max = y_true_batch.max()\n",
    "            min_val = min(min_val, batch_min)\n",
    "            max_val = max(max_val, batch_max)\n",
    "\n",
    "        data_range = max_val - min_val\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sample in test_loader:\n",
    "                x = sample['input'].to(device)\n",
    "                y_true_batch = sample['output'].cpu().numpy()\n",
    "                y_pred_batch = model(x).cpu().numpy()\n",
    "\n",
    "                for true_sample, pred_sample in zip(y_true_batch, y_pred_batch):\n",
    "                    true_sample_component = true_sample\n",
    "                    pred_sample_component = pred_sample\n",
    "                    \n",
    "                    true_sample_flat = true_sample.flatten()\n",
    "                    pred_sample_flat = pred_sample.flatten()\n",
    "\n",
    "                    # R-squared\n",
    "                    r2 = r2_score(true_sample_flat, pred_sample_flat)\n",
    "                    sample_r2_scores.append(r2)\n",
    "\n",
    "                    # Relative error\n",
    "                    absolute_error = np.linalg.norm(true_sample_flat - pred_sample_flat, 2)\n",
    "                    relative_error = absolute_error / np.linalg.norm(true_sample_flat, 2)\n",
    "                    sample_relative_errors.append(relative_error)\n",
    "\n",
    "                    # MSE\n",
    "                    mse = mean_squared_error(true_sample_flat, pred_sample_flat)\n",
    "                    sample_mses.append(mse)\n",
    "\n",
    "                    # MAE\n",
    "                    mae = mean_absolute_error(true_sample_flat, pred_sample_flat)\n",
    "                    sample_maes.append(mae)\n",
    "                    \n",
    "                    # PSNR \n",
    "                    psnr_value = psnr(true_sample_component, pred_sample_component, data_range=data_range)\n",
    "                    sample_psnr_values.append(psnr_value)\n",
    "\n",
    "                    # SSIM \n",
    "                    ssim_value = ssim(true_sample_component, pred_sample_component, data_range=data_range)\n",
    "                    sample_ssim_values.append(ssim_value)\n",
    "\n",
    "            avg_r2 = np.mean(sample_r2_scores)\n",
    "            avg_relative_error = np.mean(sample_relative_errors)\n",
    "            avg_mse = np.mean(sample_mses)\n",
    "            avg_mae = np.mean(sample_maes)\n",
    "            avg_psnr = np.mean(sample_psnr_values)\n",
    "            avg_ssim = np.mean(sample_ssim_values)\n",
    "\n",
    "        with open(results_file, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch+1, avg_r2, avg_relative_error, avg_mse, avg_mae, avg_psnr, avg_ssim])\n",
    "\n",
    "# Save final losses\n",
    "np.save(os.path.join(checkpoint_dir, 'train_losses.npy'), np.array(train_losses))\n",
    "np.save(os.path.join(checkpoint_dir, 'test_losses.npy'), np.array(test_losses))\n",
    "\n",
    "# Average Epoch Duration\n",
    "average_epoch_duration = np.mean(epoch_durations)\n",
    "print(f'Average Epoch Duration: {average_epoch_duration:.2f}s')\n",
    "\n",
    "# Total Training Time\n",
    "total_training_time = np.sum(epoch_durations)\n",
    "print(f'Total Training Time: {total_training_time:.2f}s')\n",
    "\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Test R2 Score\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(test_losses, label='Test R2 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('R2 Score')\n",
    "plt.title('Test R2 Score Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede1a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

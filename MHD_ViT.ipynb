{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f164239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from einops.layers.torch import Rearrange\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import sys\n",
    "from astropy.io import fits\n",
    "from torch.utils.data.dataset import Subset\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.transforms.functional import normalize\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "from IPython.display import HTML\n",
    "from neuralop.losses.data_losses import LpLoss, H1Loss\n",
    "from neuralop.utils import count_model_params\n",
    "import time\n",
    "from torchvision.transforms import Resize\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8974f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, hdf5_file_path):\n",
    "        self.hdf5_file_path = hdf5_file_path\n",
    "        self.hdf5_file = h5py.File(hdf5_file_path, 'r')\n",
    "        self.dataset_length = len(self.hdf5_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'x': torch.from_numpy(self.hdf5_file[f'sample_{idx}/x'][:]),\n",
    "            'y': torch.from_numpy(self.hdf5_file[f'sample_{idx}/y'][:])\n",
    "        }\n",
    "        return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b80c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=1, patch_size=16, emb_size=768):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # [B, emb_size, H/P, W/P]\n",
    "        x = x.flatten(2)  # [B, emb_size, N]\n",
    "        x = x.transpose(1, 2)  # [B, N, emb_size]\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, in_channels=1, patch_size=16, emb_size=768, depth=12, num_heads=12, output_channels=99):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(in_channels, patch_size, emb_size)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, (504//patch_size)**2, emb_size))\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=emb_size, nhead=num_heads), num_layers=depth)\n",
    "        self.to_output_shape = nn.Sequential(\n",
    "            nn.Linear(emb_size, patch_size*patch_size*output_channels),\n",
    "            Rearrange('b n (c h w) -> b c (h n) w', h=patch_size, w=patch_size),\n",
    "            Resize((504, 504), antialias=True)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x += self.pos_embed\n",
    "        x = self.transformer(x)\n",
    "        x = self.to_output_shape(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b9193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_size=504, patch_size=8, output_channels=99, d_model=128, nhead=8, num_layers=2):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.output_channels = output_channels\n",
    "        self.patch_embed = nn.Sequential(\n",
    "            nn.Conv2d(1, d_model, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b c h w -> b (h w) c')\n",
    "        )\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, d_model))\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=d_model * 2),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        #self.decoder = nn.Linear(d_model, output_channels * (image_size) * (image_size))\n",
    "        self.decoder = nn.Linear(d_model, output_channels * (image_size//2) * (image_size//2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, _, _ = x.shape\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embed\n",
    "        x = Rearrange('b n d -> n b d')(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)  # Aggregate patches\n",
    "        x = self.decoder(x)\n",
    "        #x = x.view(B, self.output_channels, self.image_size, self.image_size)\n",
    "\n",
    "        x = x.view(B, self.output_channels, self.image_size//2, self.image_size//2)\n",
    "        x = F.interpolate(x, size=(self.image_size, self.image_size), mode='bilinear', align_corners=False)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f75cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_path = \"/data.h5\"\n",
    "# Set seed for reproducibility\n",
    "seed_value = 1\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Create the custom dataset\n",
    "custom_dataset = CustomDataset(hdf5_file_path=processed_data_path)\n",
    "\n",
    "# Define the size for the test set\n",
    "test_set_size = 20\n",
    "\n",
    "# Generate random indices for the test set and corresponding training set\n",
    "all_indices = list(range(len(custom_dataset)))\n",
    "random.shuffle(all_indices)\n",
    "\n",
    "test_indices = all_indices[:test_set_size]\n",
    "train_indices = all_indices[test_set_size:]\n",
    "\n",
    "# Create training dataset using Subset\n",
    "train_dataset = Subset(custom_dataset, train_indices)\n",
    "\n",
    "# Create testing dataset using Subset\n",
    "test_dataset = Subset(custom_dataset, test_indices)\n",
    "\n",
    "# Example usage in a DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0, persistent_workers=False)\n",
    "test_loaders = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=0, persistent_workers=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab613fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = VisionTransformer()  \n",
    "model = model.to(device)\n",
    "\n",
    "n_params = count_model_params(model)\n",
    "print(f'\\nYour model has {n_params} parameters.')\n",
    "sys.stdout.flush()  # flush the stdout buffer\n",
    "\n",
    "h1loss = H1Loss(d=2)\n",
    "test_loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=2, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0d29ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "epoch_durations = []\n",
    "\n",
    "num_epochs = 300\n",
    "print_frequency = 40\n",
    "save_checkpoint_interval = 300\n",
    "checkpoint_dir = \"/result/\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # training process\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data, target = sample['x'].to(device), sample['y'].to(device)\n",
    "        output = model(data)\n",
    "        loss = h1loss(output, target)\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "        if batch_idx % print_frequency == 0:\n",
    "            print(f'Train Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(train_loader)}, Loss: {loss.item()}')\n",
    "    \n",
    "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # testing process\n",
    "    model.eval()\n",
    "    epoch_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for sample in test_loaders:\n",
    "            data, target = sample['x'].to(device), sample['y'].to(device)\n",
    "            output = model(data)\n",
    "            loss = test_loss(output, target)\n",
    "            loss = loss.mean()\n",
    "            epoch_test_loss += loss.item()\n",
    "    \n",
    "    avg_test_loss = epoch_test_loss / len(test_loaders)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    epoch_durations.append(epoch_duration)\n",
    "\n",
    "    # Updated learning rate\n",
    "    scheduler.step(avg_train_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Duration: {epoch_duration:.2f}s, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "    # save checkpoint\n",
    "    if (epoch + 1) % save_checkpoint_interval == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved at epoch {epoch+1}: {checkpoint_path}\")\n",
    "\n",
    "# average time of epoch duration\n",
    "avg_epoch_duration = np.mean(epoch_durations)\n",
    "print(f'Average Epoch Duration: {avg_epoch_duration:.2f}s')\n",
    "\n",
    "# save loss\n",
    "np.save(os.path.join(checkpoint_dir, 'train_losses.npy'), np.array(train_losses))\n",
    "np.save(os.path.join(checkpoint_dir, 'test_losses.npy'), np.array(test_losses))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(test_losses, label='Testing Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Testing Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "sample_r2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample in test_loaders:\n",
    "        x = sample['x'].to(device)\n",
    "        y_true_batch = sample['y'].numpy()\n",
    "        y_pred_batch = model(x).cpu().numpy()\n",
    "\n",
    "        # Calculate R-squared for each sample in the batch\n",
    "        for true_sample, pred_sample in zip(y_true_batch, y_pred_batch):\n",
    "            sample_r2_scores = r2_score(true_sample.flatten(), pred_sample.flatten())\n",
    "            sample_r2.append(sample_r2_scores)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize an empty list to store all sample relative errors\n",
    "sample_relative_errors = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample in test_loaders:\n",
    "        x = sample['x'].to(device)\n",
    "        y_true_batch = sample['y'].numpy()\n",
    "        y_pred_batch = model(x).cpu().numpy()\n",
    "\n",
    "        # Calculate Relative Error for each sample in the batch\n",
    "        for true_sample, pred_sample in zip(y_true_batch, y_pred_batch):\n",
    "            absolute_error = np.linalg.norm(true_sample.flatten() - pred_sample.flatten(), 2)\n",
    "            relative_error = absolute_error / (np.linalg.norm(true_sample.flatten(), 2)) \n",
    "            #print(relative_error)\n",
    "            sample_relative_errors.append(relative_error)\n",
    "            \n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "min_mse_sample = None\n",
    "sample_mses = []  # Initialize outside the loop\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample_idx, sample in enumerate(test_loaders):\n",
    "        x = sample['x'].to(device)\n",
    "        y_true_batch = sample['y'].numpy()  \n",
    "        y_pred_batch = model(x).cpu().numpy()\n",
    "\n",
    "        # Iterate through each ground truth-prediction pair in the sample\n",
    "        for true_sample, pred_sample in zip(y_true_batch, y_pred_batch):\n",
    "            mse_sample = mean_squared_error(true_sample.flatten(), pred_sample.flatten())\n",
    "            sample_mses.append(mse_sample)\n",
    "\n",
    "# Calculate overall Mean Squared Error\n",
    "overall_mse = np.mean(sample_mses)\n",
    "print(f'Overall Mean Squared Error: {overall_mse}')\n",
    "\n",
    "# Find the minimum MSE value and its index\n",
    "min_sample_mse = min(sample_mses)\n",
    "min_index = np.argmin(sample_mses)\n",
    "\n",
    "#print(f'Index of Minimum MSE: {min_index}')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize an empty list to store all sample absolute errors\n",
    "sample_absolute_errors = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample in test_loaders:\n",
    "        x = sample['x'].to(device)\n",
    "        y_true_batch = sample['y'].numpy()\n",
    "        y_pred_batch = model(x).cpu().numpy()\n",
    "\n",
    "        # Calculate Absolute Error for each sample in the batch\n",
    "        for true_sample, pred_sample in zip(y_true_batch, y_pred_batch):\n",
    "            absolute_error = np.mean(np.abs(true_sample.flatten() - pred_sample.flatten()))\n",
    "            sample_absolute_errors.append(absolute_error)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = np.mean(sample_absolute_errors)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "\n",
    "# Calculate overall Relative Error\n",
    "overall_relative_error = np.mean(sample_relative_errors)\n",
    "print(f'Overall Relative Error: {overall_relative_error}')\n",
    "\n",
    "# Calculate overall R-squared\n",
    "overall_r2 = np.mean(sample_r2)\n",
    "print(f'Overall R-squared: {overall_r2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
